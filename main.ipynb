{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a42102c-467f-4ae6-b637-cb80190e5184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.41.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (3.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.19.5)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (58.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.25.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (4.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad6c6d0-953d-47c6-b347-53cbcff6ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from models.ema import WeightEMA\n",
    "from models.utils import get_model\n",
    "from data import get_data, show_imgs\n",
    "from train import *\n",
    "from test import *\n",
    "from plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aba5146-4ce5-4c03-8bec-ca11982e9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x7fdea3032dd0>\n",
      "1\n",
      "Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# GPU stuff\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa50d36-b4c2-402b-a104-db77e79f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebc30c-359e-419a-8d3f-3d15bfb5c386",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "### Sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f495ab-337a-4317-b38b-e954de030a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "datapath = \"data/\"\n",
    "seed = 123\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using {} device'.format(device))\n",
    "\n",
    "if device == \"cuda\":\n",
    "    if 1 < torch.cuda.device_count():\n",
    "        print(f\"using multiple GPUs: n={torch.cuda.device_count()}\")\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5267b-b7a2-4841-97bc-1ee739df5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names [\"cifar10\", \"cifar100\"]\n",
    "# noise_modes [\"sym\", \"asym\", \"openset\", \"dependent\"]\n",
    "# p any probability fro noise to 1 decimal, e.g.: 0.3, 0.6\n",
    "# custom_noise if True, use custom noise (not by authors in GitHub), else from their GitHub\n",
    "# make_new_custom_noise if True, make custom noise files\n",
    "\n",
    "dataset_name = \"cifar10\"\n",
    "noise_mode = \"sym\"\n",
    "p = 0.4\n",
    "custom_noise = False\n",
    "make_new_custom_noise = False\n",
    "\n",
    "train_dataset, train_dataset_original, indices_noisy, noise_rules, test_dataset = get_data(\n",
    "    dataset_name=dataset_name,\n",
    "    datapath=datapath,\n",
    "    noise_mode=noise_mode, \n",
    "    p=p,\n",
    "    custom_noise=custom_noise,\n",
    "    make_new_custom_noise=make_new_custom_noise,\n",
    "    seed=seed)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "print(f\"dataset_name:{dataset_name}, noise_mode:{noise_mode}, noise_ratio:{indices_noisy.sum() / len(train_dataset.targets):.4f}\")\n",
    "print(\"noise_rules\")\n",
    "print(noise_rules)\n",
    "print(indices_noisy)\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "show_imgs(\n",
    "    datapath=datapath, \n",
    "    dataset_name=dataset_name, \n",
    "    train_dataset=train_dataset,\n",
    "    noise_mode=noise_mode,\n",
    "    indices_noisy=indices_noisy,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9617-59fa-4df1-8111-b6e5f54d93d1",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- openset: cifar10 and cifar100 have overlapping classes, still in openset noise it is disregarded. what if we accidentally swap a dog in cifar10 for a dog in cifar100?\n",
    "- openset: swapping transformed images in cifar10 to un-transformed in cifar100?\n",
    "- normalization parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5e40b-e1f0-406b-b248-9fa8e518534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible models:\n",
    "# 1) baseline: CE - sigma=0, optimizer_ema=None, n_epoch_label_correction_onset=0\n",
    "# 2) SLN: - 0<sigma n_epoch_label_correction_onset=0\n",
    "# 3) SLN-MO: - 0<sigma, optimizer_ema!=None, n_epoch_label_correction_onset=0\n",
    "# 4) SLN-MO-LC: - 0<sigma, optimizer_ema!=None, 0<n_epoch_label_correction_onset(=250 in paper)\n",
    "\n",
    "dataset_name = \"cifar10\"\n",
    "noise_mode = \"sym\"\n",
    "p = 0.4\n",
    "custom_noise = False\n",
    "make_new_custom_noise = False\n",
    "\n",
    "train_dataset, _, indices_noisy, noise_rules, test_dataset = get_data(\n",
    "    dataset_name=dataset_name,\n",
    "    datapath=datapath,\n",
    "    noise_mode=noise_mode, \n",
    "    p=p,\n",
    "    custom_noise=custom_noise,\n",
    "    make_new_custom_noise=make_new_custom_noise,\n",
    "    seed=seed)\n",
    "\n",
    "# get number of classes\n",
    "n_classes = len(list(train_dataset.class_to_idx.keys()))\n",
    "\n",
    "# run faster for dev\n",
    "#train_dataset.data, train_dataset.targets = train_dataset.data[:700], train_dataset.targets[:700]\n",
    "#test_dataset.data, test_dataset.targets = test_dataset.data[:600], test_dataset.targets[:600]\n",
    "\n",
    "# make targets one-hot (easier to handle in lc and sln), targets_one_hot used in lc\n",
    "targets = train_dataset.targets\n",
    "targets_one_hot, train_dataset.targets = np.eye(n_classes)[targets], np.eye(n_classes)[targets]\n",
    "targets_test = test_dataset.targets\n",
    "test_dataset.targets = np.eye(n_classes)[targets_test]\n",
    "\n",
    "# load data\n",
    "batch_size = 128\n",
    "# train_dataloader is modified if lc is used\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# train_eval_dataloader is never modified, and is used to compute the loss weights for lc\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "# test_dataloader is never modified (test dataset is not onehot yet?)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# models and opts\n",
    "model_name = \"wrn-28-2\"\n",
    "model = get_model(model_name=model_name, n_classes=n_classes, device=device)\n",
    "\n",
    "# if multi gpu\n",
    "if device == \"cuda\":\n",
    "    if 1 < torch.cuda.device_count():\n",
    "        model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "#print(model)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "model_ema = get_model(model_name=model_name, n_classes=n_classes, device=device)\n",
    "#model_ema = None\n",
    "\n",
    "if model_ema:\n",
    "    # no grads for model_ema\n",
    "    for param in model_ema.parameters():\n",
    "        param.detach_()\n",
    "\n",
    "    # if multi gpu\n",
    "    if device == \"cuda\":\n",
    "        if 1 < torch.cuda.device_count():\n",
    "            model_ema = torch.nn.DataParallel(model_ema)\n",
    "    \n",
    "    model_ema.to(device)\n",
    "    \n",
    "    optimizer_ema = WeightEMA(model, model_ema, alpha=0.999)\n",
    "else:\n",
    "    optimizer_ema = None\n",
    "\n",
    "#print(optimizer)\n",
    "\n",
    "# exp hyperparams\n",
    "n_epochs = 300\n",
    "sigma = 1.0\n",
    "n_epoch_label_correction_onset = 250\n",
    "\n",
    "# logging and tensorboard stuff\n",
    "datetime_now = datetime.datetime.now()\n",
    "exp_id = f\"exp_{datetime_now}\"\n",
    "writer = SummaryWriter(f\"runs/{exp_id}/\")\n",
    "\n",
    "# save lossess and accuracies in lists\n",
    "loss_epochs = []\n",
    "loss_noisy_epochs = []\n",
    "loss_clean_epochs = []\n",
    "accuracy_epochs = []\n",
    "loss_test_epochs = []\n",
    "accuracy_test_epochs = []\n",
    "\n",
    "ce_cond = sigma == 0 and model_ema is None and n_epochs < n_epoch_label_correction_onset\n",
    "sln_cond = 0 < sigma and model_ema is None and n_epochs < n_epoch_label_correction_onset\n",
    "sln_mo_cond = 0 < sigma and model_ema and n_epochs < n_epoch_label_correction_onset\n",
    "sln_mo_lc_cond = 0 < sigma and model_ema and n_epoch_label_correction_onset < n_epochs\n",
    "assert ce_cond or sln_cond or sln_mo_cond or sln_mo_lc_cond\n",
    "print(f\"starting training of model: {'ce' if ce_cond else 'sln' if sln_cond else 'sln_mo' if sln_mo_cond else 'sln_mo_lc' if sln_mo_lc_cond else None}\")\n",
    "print(f\"exp_id: {exp_id}\")\n",
    "\n",
    "# n_epoch starts from zero, so always adding one for logging and lc onset\n",
    "for n_epoch in range(1, n_epochs+1):\n",
    "    # label-correction\n",
    "    # if SLN-MO-LC model\n",
    "    if model_ema and n_epoch_label_correction_onset <= n_epoch:\n",
    "        # set sigma to 0, no more stochastic label noise as lc starts\n",
    "        sigma = 0\n",
    "        \n",
    "        print(f\"lc in n_epoch={n_epoch}\")\n",
    "        # keep targets one hot through lc\n",
    "        losses, softmaxes = \\\n",
    "            get_lc_params(model_ema=model_ema, train_eval_dataloader=train_eval_dataloader, device=device)\n",
    "        # normalize to [0.0, 1.0]\n",
    "        weights = torch.reshape((losses - torch.min(losses)) / (torch.max(losses) - torch.min(losses)), (len(train_dataloader.dataset), 1))\n",
    "        weights = weights.numpy()\n",
    "        preds = np.argmax(softmaxes.numpy(), axis=1).tolist()\n",
    "        preds_one_hot = np.eye(n_classes)[preds]\n",
    "        # do lc and reload training data (targets_one_hot fixed variable from above)\n",
    "        targets_one_hot_lc = weights*targets_one_hot + (1-weights)*preds_one_hot\n",
    "        train_dataset.targets = targets_one_hot_lc\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    loss_epoch, accuracy_epoch, loss_noisy_epoch, loss_clean_epoch = train_loop(\n",
    "        model=model, \n",
    "        device=device,\n",
    "        train_dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        optimizer_ema=optimizer_ema,\n",
    "        sigma=sigma,\n",
    "        n_classes=n_classes,\n",
    "        n_epoch=n_epoch,\n",
    "        n_epochs=n_epochs, \n",
    "        indices_noisy=indices_noisy\n",
    "    )\n",
    "    \n",
    "    #writer.add_scalar(\"loss/train\", loss_epoch, n_epoch)\n",
    "    #writer.add_scalar(\"loss/train_noisy\", loss_noisy_epoch, n_epoch)\n",
    "    #writer.add_scalar(\"loss/train_clean\", loss_clean_epoch, n_epoch)\n",
    "    writer.add_scalar(\"accuracy/train\", accuracy_epoch, n_epoch)\n",
    "    \n",
    "    writer.add_scalars('loss/train', {'all': loss_epoch,\n",
    "                                      'noisy': loss_noisy_epoch,\n",
    "                                      'clean': loss_clean_epoch}, n_epoch)\n",
    "    \n",
    "    loss_epochs.append(loss_epoch)\n",
    "    loss_noisy_epochs.append(loss_noisy_epoch)\n",
    "    loss_clean_epochs.append(loss_clean_epoch)\n",
    "    accuracy_epochs.append(accuracy_epoch)\n",
    "    \n",
    "    if optimizer_ema:\n",
    "        loss_test, accuracy_test = test_loop(\n",
    "            model=model_ema, \n",
    "            device=device,\n",
    "            test_dataloader=test_dataloader,\n",
    "            n_epoch=n_epoch,\n",
    "            n_epochs=n_epochs)\n",
    "        \n",
    "        writer.add_scalar(\"loss/test\", loss_test, n_epoch)\n",
    "        writer.add_scalar(\"accuracy/test\", accuracy_test, n_epoch)\n",
    "        loss_test_epochs.append(loss_test)\n",
    "        accuracy_test_epochs.append(accuracy_test)\n",
    "    \n",
    "        print(f\"epoch={n_epoch}/{n_epochs}, loss_epoch={loss_epoch:.4f}, acc_epoch={accuracy_epoch:.4f}, \"\n",
    "              f\"loss_test={loss_test:.4f}, accuracy_test={accuracy_test:.4f}\")\n",
    "    else:\n",
    "        loss_test, accuracy_test = test_loop(\n",
    "            model=model, \n",
    "            device=device,\n",
    "            test_dataloader=test_dataloader,\n",
    "            n_epoch=n_epoch,\n",
    "            n_epochs=n_epochs)\n",
    "        \n",
    "        writer.add_scalar(\"loss/test\", loss_test, n_epoch)\n",
    "        writer.add_scalar(\"accuracy/test\", accuracy_test, n_epoch)\n",
    "        loss_test_epochs.append(loss_test)\n",
    "        accuracy_test_epochs.append(accuracy_test)\n",
    "    \n",
    "        print(f\"epoch={n_epoch}/{n_epochs}, loss_epoch={loss_epoch:.4f}, acc_epoch={accuracy_epoch:.4f}, \"\n",
    "              f\"loss_test={loss_test:.4f}, accuracy_test={accuracy_test:.4f}\")\n",
    "    \n",
    "# Call flush() method to make sure that all pending events have been written to disk.\n",
    "writer.flush()\n",
    "\n",
    "# Saving\n",
    "model_save_path = Path(f\"saved_models/{exp_id}/\")\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path / \"model.pth\")\n",
    "if model_ema:\n",
    "    torch.save(model_ema.state_dict(), model_save_path / \"model_ema.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c46b5e-c421-42de-b01d-d704ecc9900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "adding inherent noise to cifar10: from paper, noise_mode:sym, p:0.4\n",
      "labels_csv_path:data/cifar10/label_noisy/sym0.4.csv, noise_rules_csv_path:data/cifar10/label_noisy/sym0.4_noise_rules.csv\n",
      "| Wide-Resnet 28x2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "noise_mode = \"sym\"\n",
    "p = 0.4\n",
    "custom_noise = False\n",
    "make_new_custom_noise = False\n",
    "\n",
    "train_dataset, train_dataset_original, indices_noisy, noise_rules, test_dataset = get_data(\n",
    "    dataset_name=dataset_name,\n",
    "    datapath=datapath,\n",
    "    noise_mode=noise_mode, \n",
    "    p=p,\n",
    "    custom_noise=custom_noise,\n",
    "    make_new_custom_noise=make_new_custom_noise,\n",
    "    seed=seed)\n",
    "\n",
    "# get number of classes\n",
    "n_classes = len(list(train_dataset.class_to_idx.keys()))\n",
    "\n",
    "# run faster for dev\n",
    "#train_dataset.data, train_dataset.targets = train_dataset.data[:700], train_dataset.targets[:700]\n",
    "#test_dataset.data, test_dataset.targets = test_dataset.data[:600], test_dataset.targets[:600]\n",
    "\n",
    "# make targets one-hot (easier to handle in lc and sln), targets_one_hot used in lc\n",
    "targets = train_dataset.targets\n",
    "targets_one_hot, train_dataset.targets = np.eye(n_classes)[targets], np.eye(n_classes)[targets]\n",
    "targets_test = test_dataset.targets\n",
    "test_dataset.targets = np.eye(n_classes)[targets_test]\n",
    "\n",
    "# load data\n",
    "batch_size = 128\n",
    "# train_dataloader is modified if lc is used\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# fully clean, for viz only\n",
    "train_original_dataloader = torch.utils.data.DataLoader(train_dataset_original, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# train_eval_dataloader is never modified, and is used to compute the loss weights for lc\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "# test_dataloader is never modified (test dataset is not onehot yet?)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# models and opts\n",
    "model_name = \"wrn-28-2\"\n",
    "model = get_model(model_name=model_name, n_classes=n_classes, device=device)\n",
    "\n",
    "# if multi gpu\n",
    "if device == \"cuda\":\n",
    "    if 1 < torch.cuda.device_count():\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "exp_id = \"exp_2021-11-26 11:18:36.051172\"\n",
    "model_load_path = Path(f\"saved_models/{exp_id}/model_ema.pth\")\n",
    "model.load_state_dict(torch.load(model_load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada3f95d-06e1-414f-bdfa-5498cb8cbcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\teval set: batch=391/391, loss_batch=2.1801, acc_batch=0.6250: 100%|██████████| 391/391 [00:11<00:00, 33.81it/s]\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, losses, softmaxes, predictions = test(model=model, device=device, dataloader=train_eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc0c53-27e0-4961-960b-abf0963ee940",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = \"exp_2021-11-26 11:18:36.051172\"\n",
    "viz_save_path = Path(f\"assets/{exp_id}\")\n",
    "title = \"c10-sln_mo_lc-sym-paper-0.4\"\n",
    "\n",
    "plot_prediction_probabilities(softmaxes=softmaxes, indices_noisy=indices_noisy, title=title, viz_save_path=viz_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb336459-96aa-4487-ac97-6660309152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = \"exp_2021-11-26 11:18:36.051172\"\n",
    "viz_save_path = Path(f\"assets/{exp_id}\")\n",
    "title = \"c10-sln_mo_lc-sym-paper-0.4\"\n",
    "\n",
    "plot_sample_dissection(\n",
    "    train_eval_dataloader=train_eval_dataloader, \n",
    "    train_original_dataloader=train_original_dataloader, \n",
    "    predictions=predictions, \n",
    "    losses=losses,\n",
    "    indices_noisy=indices_noisy,\n",
    "    title=title,\n",
    "    viz_save_path=viz_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d5fb8-e303-4cd8-bb16-d572df9639c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bad9c2-4e7e-4f68-8eb4-cf609f55d8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
