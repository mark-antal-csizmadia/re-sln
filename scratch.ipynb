{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad6c6d0-953d-47c6-b347-53cbcff6ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d08d4d1-5852-4add-a4fb-d5466b986be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7739e4-a729-46ee-92d2-ab77543bb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_params(dataset_name, datapath):\n",
    "    if dataset_name == \"cifar10\":\n",
    "        # stds are different in paper wtf\n",
    "        train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        pass\n",
    "    elif dataset_name == \"clothing1m\":\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    means = train_dataset.data.mean(axis=(0,1,2)) / 255.0\n",
    "    stds = train_dataset.data.std(axis=(0,1,2)) / 255.0\n",
    "    \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6deaae-da09-4cee-9ad8-6f16200f31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(dataset_name, **kwargs):\n",
    "    means, stds = kwargs[\"means\"], kwargs[\"stds\"]\n",
    "    \n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), \n",
    "                                              transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(means, stds)])\n",
    "\n",
    "        test_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                             transforms.Normalize(means, stds)])\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        pass\n",
    "    \n",
    "    elif dataset_name == \"clothing1m\":\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f031fd74-9aa9-492e-97a0-2a65bfcd771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(dataset_name, datapath, **kwargs):\n",
    "    train_transform, test_transform = kwargs[\"train_transform\"], kwargs[\"test_transform\"]\n",
    "    \n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=train_transform, download=True)\n",
    "        test_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=False, transform=test_transform)\n",
    "    \n",
    "    elif dataset_name == \"cifar100\":\n",
    "        pass\n",
    "    \n",
    "    elif dataset_name == \"clothing1m\":\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2919107-f8e0-42bd-b5cc-8ea3a2055e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(dataset_name, datapath):\n",
    "    means, stds = get_normalization_params(dataset_name, datapath)\n",
    "    \n",
    "    transform_params = {\"means\": means, \"stds\": stds}\n",
    "    train_transform, test_transform = get_transforms(dataset_name, **transform_params)\n",
    "    \n",
    "    transforms = {\"train_transform\": train_transform, \"test_transform\": test_transform}\n",
    "    train_dataset, test_dataset = get_splits(dataset_name, datapath, **transforms)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6577a3-6e81-4222-a4e1-3c19556ead81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "def get_datta_loaders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d927463-aac7-4905-9d93-71083f3e79e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "train_dataset, test_dataset = get_datasets(dataset_name, datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd6309d-2388-4db9-95d1-a5ce3f9cad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.49139968 0.48215841 0.44653091], std=[0.24703223 0.24348513 0.26158784])\n",
      "           )\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/cifar10\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.49139968 0.48215841 0.44653091], std=[0.24703223 0.24348513 0.26158784])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b0dafd9-08a0-49d1-9bf0-1e6fcef295cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c92dd1f-9899-4d8a-9578-b853a5ed2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test.npy\", train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e78e67d6-9d99-47f3-8e3f-21eb6c31e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = np.load(\"test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "b62a56e2-3f07-41fe-9657-901880c93363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(train_dataset.data == train_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a00b5e12-afa2-4d8d-b996-9779cf0e39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dataset_name_and_noise_mode_combos(dataset_name, noise_mode):\n",
    "    cifar10_sym_condition = dataset_name == \"cifar10\" and noise_mode == \"sym\"\n",
    "    cifar10_asym_condition = dataset_name == \"cifar10\" and noise_mode == \"asym\"\n",
    "    cifar100_sym_condition = dataset_name == \"cifar100\" and noise_mode == \"sym\"\n",
    "    cifar100_asym_condition = dataset_name == \"cifar100\" and noise_mode == \"asym\"\n",
    "    cifar10_dependent_condition = dataset_name == \"cifar10\" and noise_mode == \"dependent\"\n",
    "    cifar100_dependent_condition = dataset_name == \"cifar100\" and noise_mode == \"dependent\"\n",
    "    cifar10_openset_condition = dataset_name == \"cifar10\" and noise_mode == \"openset\"\n",
    "    \n",
    "    assert (\n",
    "        cifar10_sym_condition or cifar10_asym_condition or \\\n",
    "        cifar100_sym_condition or cifar100_asym_condition or \\\n",
    "        cifar10_dependent_condition or cifar100_dependent_condition or \\\n",
    "        cifar10_openset_condition\n",
    "    )\n",
    "    \n",
    "\n",
    "def make_inherent_label_noise(datapath, dataset_name, noise_mode, seed=None):\n",
    "    # check combos\n",
    "    assert_dataset_name_and_noise_mode_combos(dataset_name=dataset_name, noise_mode=noise_mode)\n",
    "    \n",
    "    if noise_mode in [\"sym\", \"asym\"]:\n",
    "        make_inherent_label_noise_sym_asym(datapath=datapath, dataset_name=dataset_name, noise_mode=noise_mode, seed=seed)\n",
    "    elif noise_mode == \"openset\":\n",
    "        make_inherent_label_noise_openset(datapath=datapath, dataset_name=dataset_name, noise_mode=noise_mode, seed=seed)\n",
    "    elif noise_mode == \"dependent\":\n",
    "        make_inherent_label_noise_dependent(datapath=datapath, dataset_name=dataset_name, noise_mode=noise_mode, seed=None)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "def make_inherent_label_noise_openset(datapath, dataset_name, noise_mode, seed=None):\n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    p = 0.4\n",
    "    noise_rules = make_openset_noise_rules(dataset_name=dataset_name, p=p)\n",
    "    # only one noise rule for now\n",
    "    noise_rule = noise_rules[0]\n",
    "    \n",
    "    dirty_indicator_indices_per_rule = \\\n",
    "        make_openset_noise(train_dataset=train_dataset, src=noise_rule[\"src\"], dst=noise_rule[\"dst\"], p=noise_rule[\"p\"], seed=seed)\n",
    "    #noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "    \n",
    "    labels_noisy = np.array([False for i in range(len(train_dataset))])\n",
    "    labels_noisy[dirty_indicator_indices_per_rule] = True\n",
    "\n",
    "    labels_df = pd.DataFrame(data={\"label\": train_dataset.targets, \"label_noisy\": labels_noisy})\n",
    "    labels_csv_path = f\"data/{dataset_name}/label_noisy/openset{p:.1f}_custom.csv\" \n",
    "    labels_df.to_csv(labels_csv_path, index=False)\n",
    "    print(f\"{labels_csv_path} generated\\n\")\n",
    "    \n",
    "    noise_rules_df = pd.DataFrame(data=noise_rules)\n",
    "    noise_rules_csv_path = f\"data/{dataset_name}/label_noisy/openset{p:.1f}_custom_noise_rules.csv\"\n",
    "    noise_rules_df.to_csv(noise_rules_csv_path)\n",
    "    print(f\"{noise_rules_csv_path} generated\\n\")\n",
    "\n",
    "def make_inherent_label_noise_dependent(datapath, dataset_name, noise_mode, seed=None):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def make_inherent_label_noise_sym_asym(datapath, dataset_name, noise_mode, seed=None):   \n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        train_dataset = datasets.CIFAR100(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    if noise_mode == \"sym\":\n",
    "        p = 0.4\n",
    "        noise_rules = make_sym_noise_rules(dataset_name=dataset_name, train_dataset=train_dataset, p=p)\n",
    "    elif noise_mode == \"asym\":\n",
    "        p = 0.4\n",
    "        noise_rules = make_asym_noise_rules(dataset_name=dataset_name, train_dataset=train_dataset, p=p)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    targets = torch.tensor(train_dataset.targets)\n",
    "    noisy_targets = targets.detach().clone()\n",
    "\n",
    "    for noise_rule in noise_rules:\n",
    "        indices_per_rule, dirty_indicator_indices_per_rule, noisy_targets_per_rule = \\\n",
    "            make_sym_asym_noise(train_dataset=train_dataset, src=noise_rule[\"src\"], dsts=noise_rule[\"dsts\"], p=noise_rule[\"p\"], seed=seed)\n",
    "        noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "\n",
    "    labels_df = pd.DataFrame(data={\"label\": targets.numpy(), \"label_noisy\": noisy_targets.numpy()})\n",
    "    labels_csv_path = f\"data/{dataset_name}/label_noisy/{noise_mode}{p:.1f}_custom.csv\"\n",
    "    labels_df.to_csv(labels_csv_path, index=False)\n",
    "    print(f\"{labels_csv_path} generated\\n\")\n",
    "    #print(torch.where(targets != noisy_targets)[0])\n",
    "    #print(torch.where(targets != noisy_targets)[0].size(dim=0) / targets.size(dim=0))\n",
    "    \n",
    "    noise_rules_df = pd.DataFrame(data=noise_rules)\n",
    "    noise_rules_csv_path = f\"data/{dataset_name}/label_noisy/{noise_mode}{p:.1f}_custom_noise_rules.csv\"\n",
    "    noise_rules_df.to_csv(noise_rules_csv_path)\n",
    "    print(f\"{noise_rules_csv_path} generated\\n\")\n",
    "        \n",
    "    \n",
    "def make_sym_asym_noise(train_dataset, src, dsts, p, seed=None):\n",
    "    # set seed for reprodcuibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # clean targets\n",
    "    targets = torch.tensor(train_dataset.targets)\n",
    "    # copy clean targets to noisy targets\n",
    "    noisy_targets = targets.detach().clone()\n",
    "\n",
    "    # get all labels\n",
    "    labels = torch.tensor(list(train_dataset.class_to_idx.values()))\n",
    "    labels_len = torch.tensor(labels.size(dim=0))\n",
    "\n",
    "    # get src targets indices (indices in all dataset)\n",
    "    # asym: get indices of src targets\n",
    "    indices = torch.where(targets == train_dataset.class_to_idx[src])[0]\n",
    "\n",
    "    # p_mask eg 0.4 for each src target, each target flips with probability p\n",
    "    p_mask = torch.ones_like(targets[indices]) * p\n",
    "    # flip_mask is 0s and 1s  (flip is 1s)\n",
    "    flip_mask = torch.bernoulli(input=p_mask)\n",
    "\n",
    "    # keep_mask is inverse of flip_mask (keep is 1s)\n",
    "    keep_mask = (flip_mask * (-1)) + 1\n",
    "\n",
    "    # p_mask_label is dst label probability distribution to flip to (length is number of classes), sums to 1.0\n",
    "    # asym: dst class is 1.0, all else 0.0\n",
    "    p_mask_label = torch.zeros_like(labels, dtype=torch.float)\n",
    "\n",
    "    p_mask_label[[train_dataset.class_to_idx[dst] for dst in dsts]] = 1.0 / len(dsts)\n",
    "\n",
    "    # flip_mask_label is categorical distribution with params p_mask_label for each dst class\n",
    "    flip_mask_label = torch.distributions.categorical.Categorical(p_mask_label)\n",
    "\n",
    "    # flipped_targets is dst labels for each src label that the src label can flip to\n",
    "    # for now, only one dst, so all in flip_targets is dst class label\n",
    "    flipped_targets = flip_mask_label.sample(sample_shape=targets[indices].shape)\n",
    "\n",
    "    # mask the flipped_targets to get the actually flipped instances (ones not to be flipped are 0s, ones to be flipped are dst labels)\n",
    "    masked_flipped_targets = flipped_targets * flip_mask\n",
    "\n",
    "    # mask the actual targets to keep the ones not flipped (ones not to be flipped are original labels, ones to be flipped are 0s)\n",
    "    masked_targets = targets[indices] * keep_mask\n",
    "\n",
    "    # add vectors together - kept ones remain, flipped ones are flipped\n",
    "    noisy_targets_sub = (masked_targets + masked_flipped_targets).long()\n",
    "\n",
    "    # insert into noisy_targets the flipped targets\n",
    "    noisy_targets[indices] = noisy_targets_sub\n",
    "\n",
    "    # get the indices of the noisy instances (indices in all dataset)\n",
    "    dirty_indicator_indices = torch.where(targets != noisy_targets)[0]\n",
    "    \n",
    "    return indices, dirty_indicator_indices, noisy_targets\n",
    "\n",
    "def make_openset_noise(train_dataset, src, dst, p, seed=None):\n",
    "    if dst == \"cifar100\":\n",
    "        dst_dataset = datasets.CIFAR100(os.path.join(\"data\", \"cifar100\"), train=True, download=True)\n",
    "    else:\n",
    "        raise Exception\n",
    "        \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    indices_src = np.random.choice(len(train_dataset), int(len(train_dataset) * p), replace=False)\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed + 123)\n",
    "\n",
    "    indices_dst = np.random.choice(len(train_dataset), int(len(train_dataset) * p), replace=False)\n",
    "    train_dataset.data[indices_src] = dst_dataset.data[indices_dst]\n",
    "    \n",
    "    dataset_npy_path = f\"data/{src}/label_noisy/openset{p:.1f}_custom.npy\"\n",
    "    np.save(dataset_npy_path, train_dataset.data)\n",
    "    print(f\"{dataset_npy_path} generated\\n\")\n",
    "    \n",
    "    return indices_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "edc5f7cd-7a68-42b0-bc0d-e88706c6ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "data/cifar10/label_noisy/openset0.4_custom.npy generated\n",
      "\n",
      "data/cifar10/label_noisy/openset0.4_custom.csv generated\n",
      "\n",
      "data/cifar10/label_noisy/openset0.4_custom_noise_rules.csv generated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_inherent_label_noise(datapath=datapath, dataset_name=\"cifar10\", noise_mode=\"openset\", seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0cabe209-1ab2-42d6-ba1d-d48384926db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 0, 'aquarium_fish': 1, 'baby': 2, 'bear': 3, 'beaver': 4, 'bed': 5, 'bee': 6, 'beetle': 7, 'bicycle': 8, 'bottle': 9, 'bowl': 10, 'boy': 11, 'bridge': 12, 'bus': 13, 'butterfly': 14, 'camel': 15, 'can': 16, 'castle': 17, 'caterpillar': 18, 'cattle': 19, 'chair': 20, 'chimpanzee': 21, 'clock': 22, 'cloud': 23, 'cockroach': 24, 'couch': 25, 'crab': 26, 'crocodile': 27, 'cup': 28, 'dinosaur': 29, 'dolphin': 30, 'elephant': 31, 'flatfish': 32, 'forest': 33, 'fox': 34, 'girl': 35, 'hamster': 36, 'house': 37, 'kangaroo': 38, 'keyboard': 39, 'lamp': 40, 'lawn_mower': 41, 'leopard': 42, 'lion': 43, 'lizard': 44, 'lobster': 45, 'man': 46, 'maple_tree': 47, 'motorcycle': 48, 'mountain': 49, 'mouse': 50, 'mushroom': 51, 'oak_tree': 52, 'orange': 53, 'orchid': 54, 'otter': 55, 'palm_tree': 56, 'pear': 57, 'pickup_truck': 58, 'pine_tree': 59, 'plain': 60, 'plate': 61, 'poppy': 62, 'porcupine': 63, 'possum': 64, 'rabbit': 65, 'raccoon': 66, 'ray': 67, 'road': 68, 'rocket': 69, 'rose': 70, 'sea': 71, 'seal': 72, 'shark': 73, 'shrew': 74, 'skunk': 75, 'skyscraper': 76, 'snail': 77, 'snake': 78, 'spider': 79, 'squirrel': 80, 'streetcar': 81, 'sunflower': 82, 'sweet_pepper': 83, 'table': 84, 'tank': 85, 'telephone': 86, 'television': 87, 'tiger': 88, 'tractor': 89, 'train': 90, 'trout': 91, 'tulip': 92, 'turtle': 93, 'wardrobe': 94, 'whale': 95, 'willow_tree': 96, 'wolf': 97, 'woman': 98, 'worm': 99}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12122049-7f3e-49f8-ba79-58dcd7a36aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sym_noise_rules(dataset_name, train_dataset, p=0.4):\n",
    "    if dataset_name in [\"cifar10\", \"cifar100\"]:\n",
    "        labels = list(train_dataset.class_to_idx.keys())\n",
    "        noise_rules = []\n",
    "\n",
    "        for src in labels:\n",
    "            dsts = labels.copy()\n",
    "            dsts.remove(src)\n",
    "\n",
    "            noise_rule = {\"src\":src, \"dsts\":dsts, \"p\":p}\n",
    "            noise_rules.append(noise_rule)\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "        \n",
    "    return noise_rules\n",
    "\n",
    "def make_asym_noise_rules(dataset_name, train_dataset, p=0.4):\n",
    "    if dataset_name == \"cifar10\":\n",
    "        noise_rules = [\n",
    "            {\"src\":\"truck\", \"dsts\":[\"automobile\"], \"p\":p},\n",
    "            {\"src\":\"bird\", \"dsts\":[\"airplane\"], \"p\":p},\n",
    "            {\"src\":\"cat\", \"dsts\":[\"dog\"], \"p\":p},\n",
    "            {\"src\":\"dog\", \"dsts\":[\"cat\"], \"p\":p}\n",
    "        ]\n",
    "\n",
    "    elif \"cifar100\":\n",
    "        labels = np.array(list(train_dataset.class_to_idx.keys()))\n",
    "        labels_shifted = np.roll(labels, 1)\n",
    "        noise_rules = []\n",
    "        for src, dst in zip(labels_shifted, labels):\n",
    "            noise_rule = {\"src\":src, \"dsts\":[dst], \"p\":p}\n",
    "            noise_rules.append(noise_rule)\n",
    "    else:\n",
    "        raise Exception\n",
    "        \n",
    "    return noise_rules\n",
    "\n",
    "def make_openset_noise_rules(dataset_name, p=0.4):\n",
    "    if dataset_name == \"cifar10\":\n",
    "        noise_rules = [{\"src\": dataset_name, \"dst\": \"cifar100\", \"p\": p}]\n",
    "    else:\n",
    "        raise Exception\n",
    "        \n",
    "    return noise_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebc30c-359e-419a-8d3f-3d15bfb5c386",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "### Sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "58b5267b-b7a2-4841-97bc-1ee739df5ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[{'src': 'airplane', 'dsts': ['automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'automobile', 'dsts': ['airplane', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'bird', 'dsts': ['airplane', 'automobile', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'cat', 'dsts': ['airplane', 'automobile', 'bird', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'deer', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'dog', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'dog', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'frog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'frog', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'horse', 'ship', 'truck'], 'p': 0.4}, {'src': 'horse', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'ship', 'truck'], 'p': 0.4}, {'src': 'ship', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'truck'], 'p': 0.4}, {'src': 'truck', 'dsts': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship'], 'p': 0.4}]\n",
      "tensor([    0,     1,     3,  ..., 49992, 49993, 49998])\n",
      "0.4034\n",
      "       label  label_noisy\n",
      "0          6            4\n",
      "1          9            4\n",
      "2          9            9\n",
      "3          4            5\n",
      "4          1            5\n",
      "...      ...          ...\n",
      "49995      2            2\n",
      "49996      6            6\n",
      "49997      9            9\n",
      "49998      1            9\n",
      "49999      1            1\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "noise_rules = make_sym_noise_rules(dataset_name, train_dataset)\n",
    "print(noise_rules)\n",
    "\n",
    "targets = torch.tensor(train_dataset.targets)\n",
    "noisy_targets = targets.detach().clone()\n",
    "\n",
    "for noise_rule in noise_rules:\n",
    "    indices_per_rule, dirty_indicator_indices_per_rule, noisy_targets_per_rule = \\\n",
    "        make_sym_asym_noise(train_dataset, noise_rule[\"src\"], noise_rule[\"dsts\"], noise_rule[\"p\"], seed=123)\n",
    "    noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "\n",
    "print(torch.where(targets != noisy_targets)[0])\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0) / targets.size(dim=0))\n",
    "\n",
    "df = pd.DataFrame(data={\"label\": targets.numpy(), \"label_noisy\": noisy_targets.numpy()})\n",
    "print(df)\n",
    "path_to_csv = \"data/cifar10/label_noisy/sym0.4_custom.csv\"\n",
    "df.to_csv(path_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab1e91-1590-4b49-83a3-77e44bd96750",
   "metadata": {},
   "source": [
    "### Asym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c99a152-695a-4360-929a-fc7da0786df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[{'src': 'truck', 'dsts': ['automobile'], 'p': 0.4}, {'src': 'bird', 'dsts': ['airplane'], 'p': 0.4}, {'src': 'cat', 'dsts': ['dog'], 'p': 0.4}, {'src': 'dog', 'dsts': ['cat'], 'p': 0.4}]\n",
      "tensor([    1,     6,     9,  ..., 49982, 49987, 49991])\n",
      "8068\n",
      "0.16136\n",
      "       label  label_noisy\n",
      "0          6            6\n",
      "1          9            1\n",
      "2          9            9\n",
      "3          4            4\n",
      "4          1            1\n",
      "...      ...          ...\n",
      "49995      2            2\n",
      "49996      6            6\n",
      "49997      9            9\n",
      "49998      1            1\n",
      "49999      1            1\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "noise_rules = make_asym_noise_rules(dataset_name, train_dataset)\n",
    "print(noise_rules)\n",
    "\n",
    "targets = torch.tensor(train_dataset.targets)\n",
    "noisy_targets = targets.detach().clone()\n",
    "\n",
    "for noise_rule in noise_rules:\n",
    "    indices_per_rule, dirty_indicator_indices_per_rule, noisy_targets_per_rule = \\\n",
    "        make_sym_asym_noise(train_dataset, noise_rule[\"src\"], noise_rule[\"dsts\"], noise_rule[\"p\"], seed=123)\n",
    "    noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "\n",
    "print(torch.where(targets != noisy_targets)[0])\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0))\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0) / targets.size(dim=0))\n",
    "\n",
    "df = pd.DataFrame(data={\"label\": targets.numpy(), \"label_noisy\": noisy_targets.numpy()})\n",
    "print(df)\n",
    "path_to_csv = \"data/cifar10/label_noisy/asym0.4_custom.csv\"\n",
    "df.to_csv(path_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b645fb77-d6c8-421f-8fcf-0524df422195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_noisy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  label_noisy\n",
       "0          6            6\n",
       "1          9            1\n",
       "2          9            9\n",
       "3          4            4\n",
       "4          1            1\n",
       "...      ...          ...\n",
       "49995      2            2\n",
       "49996      6            6\n",
       "49997      9            9\n",
       "49998      1            1\n",
       "49999      1            1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = pd.read_csv(path_to_csv)\n",
    "df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b28a6-57a4-4784-9552-209dfa95859b",
   "metadata": {},
   "source": [
    "### Openset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b83cebc3-8c30-4684-b5b7-a1fc3fbd8403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "make_openset_noise(dataset_name, train_dataset, p=0.4, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f28ad86f-8f4d-4c17-8c70-6203bdabaf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_dataset.data = np.load(\"data/cifar10/label_noisy/openset0.4_custom.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6427454-ff99-40d7-968d-c7538efaf096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21693510-11c2-4bbd-864e-2855ca47c643",
   "metadata": {},
   "source": [
    "### Dependent\n",
    "\n",
    "Use provided files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfcb7d-6df3-4d13-8055-52027ffdb650",
   "metadata": {},
   "source": [
    "## CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adaf73-20a3-4b6e-9732-f80642876f4c",
   "metadata": {},
   "source": [
    "### Sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ba91e0da-4382-4537-9f43-9f0393fb9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[{'src': 'apple', 'dsts': ['aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'], 'p': 0.4}, {'src': 'aquarium_fish', 'dsts': ['apple', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'], 'p': 0.4}]\n",
      "              src                                               dsts    p\n",
      "0           apple  [aquarium_fish, baby, bear, beaver, bed, bee, ...  0.4\n",
      "1   aquarium_fish  [apple, baby, bear, beaver, bed, bee, beetle, ...  0.4\n",
      "2            baby  [apple, aquarium_fish, bear, beaver, bed, bee,...  0.4\n",
      "3            bear  [apple, aquarium_fish, baby, beaver, bed, bee,...  0.4\n",
      "4          beaver  [apple, aquarium_fish, baby, bear, bed, bee, b...  0.4\n",
      "..            ...                                                ...  ...\n",
      "95          whale  [apple, aquarium_fish, baby, bear, beaver, bed...  0.4\n",
      "96    willow_tree  [apple, aquarium_fish, baby, bear, beaver, bed...  0.4\n",
      "97           wolf  [apple, aquarium_fish, baby, bear, beaver, bed...  0.4\n",
      "98          woman  [apple, aquarium_fish, baby, bear, beaver, bed...  0.4\n",
      "99           worm  [apple, aquarium_fish, baby, bear, beaver, bed...  0.4\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "tensor([    4,     8,     9,  ..., 49993, 49994, 49998])\n",
      "0.39664\n",
      "       label  label_noisy\n",
      "0         19           19\n",
      "1         29           29\n",
      "2          0            0\n",
      "3         11           11\n",
      "4          1            3\n",
      "...      ...          ...\n",
      "49995     80           80\n",
      "49996      7            7\n",
      "49997      3            3\n",
      "49998      7           41\n",
      "49999     73           73\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar100\"\n",
    "train_dataset = datasets.CIFAR100(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "noise_rules = make_sym_noise_rules(dataset_name, train_dataset)\n",
    "print(noise_rules[:2])\n",
    "\n",
    "df = pd.DataFrame(data=noise_rules)\n",
    "print(df)\n",
    "\n",
    "targets = torch.tensor(train_dataset.targets)\n",
    "noisy_targets = targets.detach().clone()\n",
    "\n",
    "for noise_rule in noise_rules:\n",
    "    indices_per_rule, dirty_indicator_indices_per_rule, noisy_targets_per_rule = \\\n",
    "        make_sym_asym_noise(train_dataset, noise_rule[\"src\"], noise_rule[\"dsts\"], noise_rule[\"p\"], seed=None)\n",
    "    noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "\n",
    "print(torch.where(targets != noisy_targets)[0])\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0) / targets.size(dim=0))\n",
    "\n",
    "df = pd.DataFrame(data={\"label\": targets.numpy(), \"label_noisy\": noisy_targets.numpy()})\n",
    "print(df)\n",
    "path_to_csv = \"data/cifar100/label_noisy/sym0.4_custom.csv\"\n",
    "df.to_csv(path_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302a55b-3e36-46ff-816a-38b0255e6b12",
   "metadata": {},
   "source": [
    "### Asym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b667c5f5-1a44-40fb-9ed5-e5b74b91f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "              src             dsts    p\n",
      "0            worm          [apple]  0.4\n",
      "1           apple  [aquarium_fish]  0.4\n",
      "2   aquarium_fish           [baby]  0.4\n",
      "3            baby           [bear]  0.4\n",
      "4            bear         [beaver]  0.4\n",
      "..            ...              ...  ...\n",
      "95       wardrobe          [whale]  0.4\n",
      "96          whale    [willow_tree]  0.4\n",
      "97    willow_tree           [wolf]  0.4\n",
      "98           wolf          [woman]  0.4\n",
      "99          woman           [worm]  0.4\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5116/3079572786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar100\"\n",
    "train_dataset = datasets.CIFAR100(os.path.join(datapath, dataset_name), train=True, transform=transforms.ToTensor(), download=True)\n",
    "noise_rules = make_asym_noise_rules(dataset_name, train_dataset)\n",
    "\n",
    "df = pd.DataFrame(data=noise_rules)\n",
    "print(df)\n",
    "raise\n",
    "\n",
    "targets = torch.tensor(train_dataset.targets)\n",
    "noisy_targets = targets.detach().clone()\n",
    "\n",
    "for noise_rule in noise_rules:\n",
    "    indices_per_rule, dirty_indicator_indices_per_rule, noisy_targets_per_rule = \\\n",
    "        make_sym_asym_noise(train_dataset, noise_rule[\"src\"], noise_rule[\"dsts\"], noise_rule[\"p\"], seed=None)\n",
    "    noisy_targets[indices_per_rule] = noisy_targets_per_rule[indices_per_rule]\n",
    "\n",
    "print(torch.where(targets != noisy_targets)[0])\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0))\n",
    "print(torch.where(targets != noisy_targets)[0].size(dim=0) / targets.size(dim=0))\n",
    "\n",
    "df = pd.DataFrame(data={\"label\": targets.numpy(), \"label_noisy\": noisy_targets.numpy()})\n",
    "print(df)\n",
    "path_to_csv = \"data/cifar100/label_noisy/asym0.4_custom.csv\"\n",
    "df.to_csv(path_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013acc0-c2b6-4fba-9e70-11f45ae41deb",
   "metadata": {},
   "source": [
    "### Openset\n",
    "\n",
    "No such combination\n",
    "\n",
    "### Dependent\n",
    "\n",
    "Use provided files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097e1b-e03a-453c-bdf6-32c9e9121e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
